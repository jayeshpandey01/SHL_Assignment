{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InfiSmile/SHL_Assignment/blob/main/Muskan_SHL_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4_YcEOyV9Qj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWIkmMMamyTO"
      },
      "source": [
        "#Text + Audio + Rules Ensemble\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpKfQwcDWBcZ"
      },
      "source": [
        "At first, I just combined text and audio features and trained one model. Then I tried using only text models like Sentence Transformer and DeBERTa to see how well they perform, but that showed how important audio features actually are. I used NVIDIA’s Parakeet-TDT-0.6B-v2 model to transcribe the audios for better text data.\n",
        "\n",
        "Later, I explored few articles that I have mentioned later and found that ensembling can give better results. So, I built a setup where text, audio, and rule-based models are trained separately and then combined using NNLS and confidence-based blending. This way, each model contributes its strengths, text for meaning, audio for tone, and rules for structure ,making the final predictions more reliable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PifMgVZGXw28"
      },
      "source": [
        "*Installation*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   !pip install -U transformers huggingface_hub\n",
        "2.   !pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d2Azo82X_F6"
      },
      "source": [
        "#Imports and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-TgDEIsXmC1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, math, warnings, random, re\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
        "\n",
        "#Ensemble Methods\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_AVAILABLE = True\n",
        "except Exception:\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    LGB_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "    SWA_AVAILABLE = True\n",
        "except Exception:\n",
        "    SWA_AVAILABLE = False\n",
        "\n",
        "#---For Audio------------\n",
        "import librosa, whisper\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "#Config\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TEXT_MODEL = \"microsoft/deberta-v3-small\"\n",
        "MAX_LEN = 256\n",
        "BS = 8\n",
        "EPOCHS_TEXT = 5\n",
        "EPOCHS_AUDIO = 8\n",
        "CLIP_RANGE = (0.0, 5.0)\n",
        "USE_ZSCORE = True\n",
        "AUDIO_SR = 16000\n",
        "\n",
        "# Paths\n",
        "TRAIN_CSV = \"csvs/train.csv\"\n",
        "TEST_CSV  = \"csvs/test.csv\"\n",
        "AUDIO_ROOT = \"audios\"\n",
        "OUT_DIR = \"output\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "# ================================\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s);\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "\n",
        "set_seed()\n",
        "print(f\"Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0EW9LZIYwYk"
      },
      "source": [
        "# Utility Functions\n",
        "$\\text{MAE} = \\frac{1}{N}\\sum_{i=1}^{N} |y_i - \\hat{y}_i|$\n",
        "\n",
        "$\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$\n",
        "\n",
        "$r (Pearson correlation coefficient)  = \\frac{\\sum_{i=1}^{N} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}\n",
        "{\\sqrt{\\sum_{i=1}^{N} (y_i - \\bar{y})^2} \\sqrt{\\sum_{i=1}^{N} (\\hat{y}_i - \\bar{\\hat{y}})^2}}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UX7tfNnbY0n5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def metrics(y, p):\n",
        "    mae = mean_absolute_error(y, p)\n",
        "    rmse = math.sqrt(mean_squared_error(y, p))\n",
        "    r = pearsonr(y, p)[0] if len(np.unique(y)) > 1 else np.nan\n",
        "    return mae, rmse, r\n",
        "\n",
        "#As mentioned in the assignment that it should be in the range 0 to 5\n",
        "def clip01_5(x):\n",
        "    return np.clip(x, CLIP_RANGE[0], CLIP_RANGE[1])\n",
        "\n",
        "def rank_scale(x: np.ndarray) -> np.ndarray:\n",
        "    idx = np.argsort(np.argsort(x))\n",
        "    return idx.astype(np.float32) / max(1, len(x)-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qn1TJl4ZHgs"
      },
      "source": [
        "# Transcript Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o9XPOYvQZG92"
      },
      "outputs": [],
      "source": [
        "def clean_text(s: str) -> str:\n",
        "    s = re.sub(r\"\\b(\\w+)(\\s+\\1\\b)+\", r\"\\1\", s, flags=re.I)  # Since there are repeated tokens like I like I like\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "#Several disfluencies in the audio (explored few of them)\n",
        "DISFLUENCIES = {\"uh\",\"um\",\"erm\",\"hmm\",\"you know\",\"like\",\"sort of\"}\n",
        "\n",
        "def extract_rule_feats(texts: list[str]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for t in texts:\n",
        "        s = t or \"\"\n",
        "        tokens = s.split()\n",
        "        tok_n = len(tokens)\n",
        "        chars = len(s)\n",
        "        avg_tok = (chars / max(1, tok_n))\n",
        "        commas = s.count(\",\"); periods = s.count(\".\"); qmarks = s.count(\"?\"); exc = s.count(\"!\")\n",
        "        caps_ratio = sum(ch.isupper() for ch in s) / max(1, len(s))\n",
        "        repeats = sum(1 for i in range(1, tok_n) if tokens[i].lower()==tokens[i-1].lower())\n",
        "        disfluency_hits = sum(1 for w in DISFLUENCIES if w in s.lower())\n",
        "        rows.append(dict(\n",
        "            tok_n=tok_n, chars=chars, avg_tok=avg_tok,\n",
        "            commas=commas, periods=periods, qmarks=qmarks, exclam=exc,\n",
        "            caps_ratio=caps_ratio, repeats=repeats, disfluencies=disfluency_hits\n",
        "        ))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "## Rule-based feature engineering: These are later used by a simple regressor(Light GBM) for rule based model\n",
        "def enrich_rules(df):\n",
        "    df = df.copy()\n",
        "    df[\"punct_rate\"] = (df[\"commas\"]+df[\"periods\"]+df[\"qmarks\"]+df[\"exclam\"]) / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"repeat_rate\"] = df[\"repeats\"] / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"disfluency_rate\"] = df[\"disfluencies\"] / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"chars_per_tok\"] = df[\"chars\"] / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"caps_x_punct\"] = df[\"caps_ratio\"] * df[\"punct_rate\"]\n",
        "    df[\"avgTok_x_punct\"] = df[\"avg_tok\"] * df[\"punct_rate\"]\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er9yVZZwaAKB"
      },
      "source": [
        "#Audio Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r71t4bdQaG1z"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AudioFeaturizer:\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = whisper.load_model(\"tiny\", device=self.device)\n",
        "\n",
        "    def _encode(self, wav, sr):\n",
        "        if sr != 16000: #frequency\n",
        "            wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
        "        wav = whisper.pad_or_trim(torch.tensor(wav)) #Since whisper expects a fixed-length audio input.\n",
        "        #Convert waveform to log-Mel spectrogram for Whisper\n",
        "        mel = whisper.log_mel_spectrogram(wav).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            #Encoding features\n",
        "            hs = self.model.encoder(mel.unsqueeze(0))  # [1,T,384]\n",
        "        #averaging over time steps to obtain one fixed-size 384-dim audio embedding\n",
        "        return hs.squeeze(0).float().cpu().mean(dim=0)  # [384]\n",
        "\n",
        "    def __call__(self, wav_path: str):\n",
        "        ''' Load, normalize, and clean audio, then extract basic prosodic features (duration, RMS, ZCR) '''\n",
        "        wav, sr = librosa.load(wav_path, sr=AUDIO_SR, mono=True)\n",
        "        wav = librosa.util.normalize(wav)\n",
        "        wav, _ = librosa.effects.trim(wav, top_db=20)\n",
        "        dur = len(wav) / AUDIO_SR\n",
        "        rms = float(librosa.feature.rms(y=wav).mean())\n",
        "        zcr = float(librosa.feature.zero_crossing_rate(y=wav).mean())\n",
        "        # pitch features\n",
        "        try:\n",
        "            f0 = librosa.yin(wav, fmin=80, fmax=400, sr=AUDIO_SR)\n",
        "            f0 = f0[np.isfinite(f0)]\n",
        "            f0_mean = float(np.nanmean(f0)) if f0.size else 0.0\n",
        "            f0_std  = float(np.nanstd(f0))  if f0.size else 0.0\n",
        "            voiced_ratio = float(np.mean((f0 > 0).astype(float))) if f0.size else 0.0\n",
        "        except Exception:\n",
        "            f0_mean = f0_std = voiced_ratio = 0.0\n",
        "        enc = self._encode(wav, sr)  # [384]\n",
        "        vec = torch.cat([enc, torch.tensor([dur, rms, zcr, f0_mean, f0_std, voiced_ratio], dtype=torch.float32)], dim=0)  # [390]\n",
        "        return vec.numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCbcsl6DbWWg"
      },
      "source": [
        "#Datasets\n",
        "\n",
        "\n",
        "1.   Text Model: \"microsoft/deberta-v3-small\"\n",
        "2.   Audio Model: openai- whisper tiny model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Uld6nNPcbaYB"
      },
      "outputs": [],
      "source": [
        "#For loading transcripts\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts): self.texts = texts\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i): return {\"text\": self.texts[i]}\n",
        "\n",
        "def text_collate(batch): return {\"text\": [b[\"text\"] for b in batch]}\n",
        "\n",
        "class TextRegressor(nn.Module):\n",
        "    '''  Text regression model using DeBERTa encoder and MLP head (CLS + mean pooled features)\n",
        "       [CLS] captures global sentence-level semantics learned during pretraining whereas,\n",
        "     Mean pooling adds information from all tokens, giving a more smoother context '''\n",
        "    def __init__(self, model_name=TEXT_MODEL):\n",
        "        super().__init__()\n",
        "        self.tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
        "        self.txt = AutoModel.from_pretrained(model_name, trust_remote_code=False)\n",
        "        hid = self.txt.config.hidden_size\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(2*hid, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, texts):\n",
        "        tok = self.tok(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
        "        out = self.txt(**tok).last_hidden_state  # [B,L,H]\n",
        "        cls  = out[:, 0]\n",
        "        mean = out.mean(dim=1)\n",
        "        emb  = torch.cat([cls, mean], dim=1)\n",
        "        return self.head(emb).squeeze(-1)\n",
        "\n",
        "#Since we would be using Layer-Wise Learning Rate Decay (LLRD) —\n",
        "#we need to assign a different learning rate to each transformer layer.\n",
        "def _get_layers(model):\n",
        "    #Check if the model has encoder.layer Like deBERTa\n",
        "    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
        "        return list(model.encoder.layer)\n",
        "    if hasattr(model, \"transformer\") and hasattr(model.transformer, \"layer\"):\n",
        "        return list(model.transformer.layer)\n",
        "    raise AttributeError(\"Unknown transformer layers path.\")\n",
        "\n",
        "#It helps fine-tune large pretrained models more safely like upper layers adapts to the new task\n",
        "#Lower layers changes slowly. So basically it shouldn't forget it's pretrained knowledge\n",
        "def llrd_params(model: TextRegressor, base_lr=3e-5, head_lr=1e-3, decay=0.9):\n",
        "    groups = [{\"params\": model.head.parameters(), \"lr\": head_lr}]\n",
        "    layers = _get_layers(model.txt)\n",
        "    lr = base_lr\n",
        "    for i in reversed(range(len(layers))):\n",
        "        groups.append({\"params\": layers[i].parameters(), \"lr\": lr})\n",
        "        lr *= decay\n",
        "    if hasattr(model.txt, \"embeddings\"):\n",
        "        groups.append({\"params\": model.txt.embeddings.parameters(), \"lr\": lr})\n",
        "    return groups\n",
        "\n",
        "#Small MLP model for audio embeddings which predicts a single regression value (as we need a value between 0 to 5)\n",
        "class AudioMLP(nn.Module):\n",
        "    def __init__(self, in_dim=390):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(512, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).squeeze(-1)\n",
        "\n",
        "# R-Drop loss: combines supervised loss with consistency loss (for stability under dropout)\n",
        "def rdrop_loss(pred1, pred2, target, base_loss, alpha=2.0):\n",
        "    sup = base_loss(pred1, target) + base_loss(pred2, target)\n",
        "    cons = torch.mean((pred1 - pred2) ** 2)\n",
        "    return 0.5 * sup + alpha * cons\n",
        "\n",
        "# Maintains a moving average of model weights for smoother, more stable training\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "    def update(self, model):\n",
        "        with torch.no_grad():\n",
        "            for k, v in model.state_dict().items():\n",
        "                self.shadow[k].mul_((self.decay)).add_(v.detach(), alpha=1-self.decay)\n",
        "    def apply_to(self, model):\n",
        "        model.load_state_dict(self.shadow, strict=True)\n",
        "\n",
        "# It helps regularize the model . So basically it blends random pairs of samples and labels\n",
        "def mixup(x, y, alpha=0.2):\n",
        "    if alpha <= 0: return x, y\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    x_mix = lam * x + (1-lam) * x[idx]\n",
        "    y_mix = lam * y + (1-lam) * y[idx]\n",
        "    return x_mix, y_mix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QOk2jlFeerg"
      },
      "source": [
        "#Training Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "20Zw1bLwemm3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# - Train text regression model (DeBERTa + MLP) using gradual unfreezing + EMA + R-Drop\n",
        "# - Starts by freezing encoder layers, then unfreezes deeper ones each epoch\n",
        "# - Applies layer-wise learning rate decay (LLRD)\n",
        "# - Uses R-Drop for regularization and EMA for stable weight tracking\n",
        "def train_text(texts, y, idx_tr, idx_va, epochs=EPOCHS_TEXT):\n",
        "    model = TextRegressor().to(DEVICE)\n",
        "    for p in model.txt.parameters(): p.requires_grad = False\n",
        "\n",
        "    opt = torch.optim.AdamW(llrd_params(model, base_lr=3e-5, head_lr=1e-3, decay=0.9), weight_decay=1e-3)\n",
        "    loss_fn = nn.HuberLoss(delta=1.0)\n",
        "    steps = math.ceil(len(idx_tr)/BS) * epochs\n",
        "    sch = get_cosine_schedule_with_warmup(opt, int(0.1*steps), steps)\n",
        "    ema = EMA(model, decay=0.999)\n",
        "\n",
        "    mu = float(y[idx_tr].mean()); sigma = float(y[idx_tr].std() + 1e-6) if USE_ZSCORE else 1.0\n",
        "    def z(v): return (v - mu) / sigma if USE_ZSCORE else v\n",
        "    def uz(v): return v * sigma + mu if USE_ZSCORE else v\n",
        "\n",
        "    layers = _get_layers(model.txt)\n",
        "    for ep in range(epochs):\n",
        "        layers_to_unfreeze = min(2 + ep, len(layers))\n",
        "        for p in model.txt.parameters(): p.requires_grad = False\n",
        "        for i in range(len(layers) - layers_to_unfreeze, len(layers)):\n",
        "            for p in layers[i].parameters(): p.requires_grad = True\n",
        "        if hasattr(model.txt, \"embeddings\"):\n",
        "            for p in model.txt.embeddings.parameters(): p.requires_grad = False\n",
        "\n",
        "        model.train()\n",
        "        order = np.random.permutation(idx_tr)\n",
        "        for start in range(0, len(order), BS):\n",
        "            bix = order[start:start+BS]\n",
        "            b_texts = [texts[i] for i in bix]\n",
        "            b_labels = torch.tensor(z(y[bix]), dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            preds1 = model(b_texts)\n",
        "            preds2 = model(b_texts)\n",
        "            loss = rdrop_loss(preds1, preds2, b_labels, loss_fn, alpha=2.0)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sch.step()\n",
        "            ema.update(model)\n",
        "\n",
        "    ema.apply_to(model)\n",
        "\n",
        "    model.eval()\n",
        "    va_texts = [texts[i] for i in idx_va]\n",
        "    va_preds = []\n",
        "    with torch.no_grad():\n",
        "        for s in range(0, len(va_texts), BS):\n",
        "            chunk = va_texts[s:s+BS]\n",
        "            p = model(chunk).detach().cpu().numpy()\n",
        "            va_preds.append(p)\n",
        "    va_preds = uz(np.concatenate(va_preds))\n",
        "    return va_preds, model, (mu, sigma)\n",
        "\n",
        "def train_audio(vecs, y, idx_tr, idx_va, epochs=EPOCHS_AUDIO):\n",
        "    model = AudioMLP(in_dim=vecs.shape[1]).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "    loss_fn = nn.HuberLoss(delta=1.0)\n",
        "\n",
        "    mu = float(y[idx_tr].mean()); sigma = float(y[idx_tr].std() + 1e-6) if USE_ZSCORE else 1.0\n",
        "    def z(v): return (v - mu) / sigma if USE_ZSCORE else v\n",
        "    def uz(v): return v * sigma + mu if USE_ZSCORE else v\n",
        "\n",
        "    use_swa = SWA_AVAILABLE and (epochs >= 3)\n",
        "    if use_swa:\n",
        "        swa_model = AveragedModel(model)\n",
        "        swa_start = epochs - 3\n",
        "        swa_scheduler = SWALR(opt, swa_lr=5e-4)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        order = np.random.permutation(idx_tr)\n",
        "        for s in range(0, len(order), BS):\n",
        "            bix = order[s:s+BS]\n",
        "            bx = torch.tensor(vecs[bix], dtype=torch.float32, device=DEVICE)\n",
        "            by = torch.tensor(z(y[bix]), dtype=torch.float32, device=DEVICE)\n",
        "            bx, by = mixup(bx, by, alpha=0.15)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            pred = model(bx)\n",
        "            loss = loss_fn(pred, by)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            if use_swa and ep >= swa_start:\n",
        "                swa_model.update_parameters(model)\n",
        "                swa_scheduler.step()\n",
        "\n",
        "    if use_swa:\n",
        "        for p, sp in zip(model.parameters(), swa_model.parameters()):\n",
        "            p.data.copy_(sp.data)\n",
        "\n",
        "    model.eval()\n",
        "    va_preds = []\n",
        "    with torch.no_grad():\n",
        "        for s in range(0, len(idx_va), BS):\n",
        "            bix = idx_va[s:s+BS]\n",
        "            bx = torch.tensor(vecs[bix], dtype=torch.float32, device=DEVICE)\n",
        "            p = model(bx).detach().cpu().numpy()\n",
        "            va_preds.append(p)\n",
        "    va_preds = uz(np.concatenate(va_preds))\n",
        "    return va_preds, model, (mu, sigma)\n",
        "\n",
        "\n",
        "#Monte Carlo (MC) dropout inference for text model. Keep dropout active during inference.\n",
        "def mc_pred_text(model, texts, n=8):\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for m in model.head.modules():\n",
        "        if isinstance(m, nn.Dropout): m.train()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n):\n",
        "            out = []\n",
        "            for i in range(0, len(texts), BS):\n",
        "                batch = texts[i:i+BS]\n",
        "                tokd = model.tok(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
        "                hidden = model.txt(**tokd).last_hidden_state\n",
        "                cls, mean = hidden[:,0], hidden.mean(dim=1)\n",
        "                emb = torch.cat([cls, mean], dim=1)\n",
        "                p = model.head(emb).squeeze(-1).detach().cpu().numpy()\n",
        "                out.append(p)\n",
        "            preds.append(np.concatenate(out))\n",
        "    preds = np.stack(preds, 0)\n",
        "    return preds.mean(0), preds.std(0)\n",
        "\n",
        "#Monte Carlo (MC) dropout inference for audio model\n",
        "def mc_pred_audio(model, X, n=8):\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout): m.train()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n):\n",
        "            out = []\n",
        "            for i in range(0, len(X), BS):\n",
        "                b = torch.tensor(X[i:i+BS], dtype=torch.float32, device=DEVICE)\n",
        "                out.append(model(b).detach().cpu().numpy())\n",
        "            preds.append(np.concatenate(out))\n",
        "    preds = np.stack(preds, 0)\n",
        "    return preds.mean(0), preds.std(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcouyh_NfkBD"
      },
      "source": [
        "# Dataloading and Feature EXtraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(filepath):\n",
        "    audio, sr = librosa.load(filepath, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "    return np.mean(mfcc.T, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "label = []\n",
        "for file_name in os.listdir('audios/train'):\n",
        "    label.append(f'audios/train/{file_name}')\n",
        "\n",
        "# Correct way to create DataFrame\n",
        "df_file = pd.DataFrame(label, columns=['filename'])\n",
        "train = pd.read_csv('csvs/train.csv')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_file['filename_clean'] = df_file['filename'].apply(lambda x: x.split('/')[-1].replace('.wav', ''))\n",
        "\n",
        "df_merged = pd.merge(df_file, train, left_on='filename_clean', right_on='filename', how='left')\n",
        "\n",
        "df_merged.drop(columns=['filename_clean'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_merged.drop(columns='filename_y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename_x</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>audios/train/audio_1.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>audios/train/audio_10.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>audios/train/audio_100.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>audios/train/audio_101.wav</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>audios/train/audio_102.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>audios/train/audio_95.wav</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>audios/train/audio_96.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>audios/train/audio_97.wav</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>audios/train/audio_98.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>audios/train/audio_99.wav</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>409 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     filename_x  label\n",
              "0      audios/train/audio_1.wav    3.0\n",
              "1     audios/train/audio_10.wav    3.0\n",
              "2    audios/train/audio_100.wav    3.0\n",
              "3    audios/train/audio_101.wav    3.5\n",
              "4    audios/train/audio_102.wav    3.0\n",
              "..                          ...    ...\n",
              "404   audios/train/audio_95.wav    2.0\n",
              "405   audios/train/audio_96.wav    3.0\n",
              "406   audios/train/audio_97.wav    2.0\n",
              "407   audios/train/audio_98.wav    3.0\n",
              "408   audios/train/audio_99.wav    3.5\n",
              "\n",
              "[409 rows x 2 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename_x</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>audios/train/audio_1.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>audios/train/audio_10.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>audios/train/audio_100.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>audios/train/audio_101.wav</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>audios/train/audio_102.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>audios/train/audio_95.wav</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>audios/train/audio_96.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>audios/train/audio_97.wav</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>audios/train/audio_98.wav</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>audios/train/audio_99.wav</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>409 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     filename_x  label\n",
              "0      audios/train/audio_1.wav    3.0\n",
              "1     audios/train/audio_10.wav    3.0\n",
              "2    audios/train/audio_100.wav    3.0\n",
              "3    audios/train/audio_101.wav    3.5\n",
              "4    audios/train/audio_102.wav    3.0\n",
              "..                          ...    ...\n",
              "404   audios/train/audio_95.wav    2.0\n",
              "405   audios/train/audio_96.wav    3.0\n",
              "406   audios/train/audio_97.wav    2.0\n",
              "407   audios/train/audio_98.wav    3.0\n",
              "408   audios/train/audio_99.wav    3.5\n",
              "\n",
              "[409 rows x 2 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = df.copy()\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "NoBackendError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
            "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'audios/train/audio_1.wav': Format not recognised.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilename_x\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_features\u001b[39m(filepath):\n\u001b[1;32m----> 5\u001b[0m     audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     mfcc \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39msr, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(mfcc\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\audioread\\__init__.py:131\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
            "\u001b[1;31mNoBackendError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "X = np.vstack(train_df[\"filename_x\"].apply(extract_features))\n",
        "y = train_df[\"label\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7IY89ulffm_Y"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# Clean transcripts\n",
        "# train_df[\"text\"] = train_df[\"text\"].map(clean_text)\n",
        "# test_df[\"text\"]  = test_df[\"text\"].map(clean_text)\n",
        "\n",
        "y_all = train_df[\"label\"].values.astype(np.float32)\n",
        "\n",
        "# print(\"Extracting rule features...\")\n",
        "# train_rules = enrich_rules(extract_rule_feats(train_df[\"text\"].tolist()))\n",
        "# test_rules  = enrich_rules(extract_rule_feats(test_df[\"text\"].tolist()))\n",
        "\n",
        "# print(\"Extracting audio vectors \")\n",
        "# fe = AudioFeaturizer(device=None)  # auto-select device\n",
        "# def wav_path(mode, fn): return f\"{AUDIO_ROOT}/{mode}/{fn}.wav\"\n",
        "# train_audio_vecs = np.stack([fe(wav_path(\"train\", fn)) for fn in train_df[\"filename\"]])  # [N,390]\n",
        "# test_audio_vecs  = np.stack([fe(wav_path(\"test\",  fn)) for fn in test_df[\"filename\"]])\n",
        "# # print(\"Shapes:\", train_audio_vecs.shape, test_audio_vecs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqMpYAp8f1BE"
      },
      "source": [
        "# K-Fold Training\n",
        "\n",
        "\n",
        "1. The training loop performs 5-fold cross-validation, splitting data into train and validation sets in each fold.\n",
        "2. It trains separate text, audio, and rule-based models, generates out-of-fold predictions for validation, and also makes test predictions for each fold to later average for final results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-computed audio features...\n",
            "✓ Loaded: train=(409, 390), test=(197, 390)\n"
          ]
        }
      ],
      "source": [
        "# Load pre-computed audio features\n",
        "print(\"Loading pre-computed audio features...\")\n",
        "train_audio_vecs = np.load('train_audio_features.npy')\n",
        "test_audio_vecs = np.load('test_audio_features.npy')\n",
        "print(f\"✓ Loaded: train={train_audio_vecs.shape}, test={test_audio_vecs.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "t4KZfe-xf4ui"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Fold 1/5 =====\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'text'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m y_all\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train text regression model and get validation preds\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m text_va_pred, text_model, (mu_t, sig_t) \u001b[38;5;241m=\u001b[39m train_text(\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues, y, tr_idx, va_idx, epochs\u001b[38;5;241m=\u001b[39mEPOCHS_TEXT)\n\u001b[0;32m     25\u001b[0m oof_text[va_idx] \u001b[38;5;241m=\u001b[39m text_va_pred\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#  MC Dropout to get mean + std predictions for test set\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\piyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ],
      "source": [
        "#Prepare stratified folds based on target distribution\n",
        "bins = pd.qcut(y_all, q=min(10, max(2, len(y_all)//30)), labels=False, duplicates=\"drop\")\n",
        "K = 5\n",
        "kf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n",
        "#Initialize arrays for out-of-fold (OOF) and test predictions\n",
        "oof_text  = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_audio = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_rules = np.zeros(len(train_df), dtype=np.float32)\n",
        "#Store predictions and uncertainties for each fold\n",
        "test_text_mean  = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_text_std   = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_audio_mean = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_audio_std  = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_rules_preds = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "\n",
        " #K-Fold training loop\n",
        "fold_idx = 0\n",
        "for tr_idx, va_idx in kf.split(train_df, bins):\n",
        "    fold_idx += 1\n",
        "    print(f\"\\n===== Fold {fold_idx}/{K} =====\")\n",
        "    y = y_all\n",
        "\n",
        "    # Train text regression model and get validation preds\n",
        "    text_va_pred, text_model, (mu_t, sig_t) = train_text(train_df[\"text\"].values, y, tr_idx, va_idx, epochs=EPOCHS_TEXT)\n",
        "    oof_text[va_idx] = text_va_pred\n",
        "\n",
        "    #  MC Dropout to get mean + std predictions for test set\n",
        "    mean_t, std_t = mc_pred_text(text_model, test_df[\"text\"].tolist(), n=8)\n",
        "    mean_t = mean_t * (sig_t if USE_ZSCORE else 1.0) + (mu_t if USE_ZSCORE else 0.0)\n",
        "    test_text_mean[fold_idx-1] = mean_t\n",
        "    test_text_std[fold_idx-1]  = std_t\n",
        "\n",
        "    # Train audio MLP model and get validation preds\n",
        "    audio_va_pred, audio_model, (mu_a, sig_a) = train_audio(train_audio_vecs, y, tr_idx, va_idx, epochs=EPOCHS_AUDIO)\n",
        "    oof_audio[va_idx] = audio_va_pred\n",
        "\n",
        "    #Run MC Dropout for test audio embeddings\n",
        "    mean_a, std_a = mc_pred_audio(audio_model, test_audio_vecs, n=8)\n",
        "    mean_a = mean_a * (sig_a if USE_ZSCORE else 1.0) + (mu_a if USE_ZSCORE else 0.0)\n",
        "    test_audio_mean[fold_idx-1] = mean_a\n",
        "    test_audio_std[fold_idx-1]  = std_a\n",
        "\n",
        "    # Rule based model\n",
        "    X_tr_rules = train_rules.iloc[tr_idx].values\n",
        "    y_tr = y[tr_idx].astype(float)\n",
        "    X_va = train_rules.iloc[va_idx].values\n",
        "    y_va = y[va_idx].astype(float)\n",
        "\n",
        "    if LGB_AVAILABLE:\n",
        "        feature_cols = list(train_rules.columns)\n",
        "        neg_names = {\"repeats\", \"disfluencies\", \"caps_ratio\", \"repeat_rate\", \"disfluency_rate\", \"caps_x_punct\"}\n",
        "        monotone_constraints = [(-1 if c in neg_names else 0) for c in feature_cols]\n",
        "\n",
        "        lgbm = lgb.LGBMRegressor(\n",
        "            n_estimators=1400,\n",
        "            learning_rate=0.02,\n",
        "            num_leaves=63,\n",
        "            min_child_samples=25,\n",
        "            subsample=0.85,\n",
        "            colsample_bytree=0.85,\n",
        "            reg_alpha=0.05,\n",
        "            reg_lambda=0.05,\n",
        "            random_state=SEED,\n",
        "            verbosity=-1,\n",
        "            monotone_constraints=monotone_constraints,\n",
        "        )\n",
        "        # Train LGBM and predict validation\n",
        "        lgbm.fit(\n",
        "            X_tr_rules, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            eval_metric=\"l2\",\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=50),\n",
        "                lgb.log_evaluation(period=0),\n",
        "            ],\n",
        "        )\n",
        "        best_iter = getattr(lgbm, \"best_iteration_\", None)\n",
        "        oof_rules[va_idx] = lgbm.predict(X_va, num_iteration=best_iter)\n",
        "        test_rules_preds[fold_idx-1] = lgbm.predict(test_rules.values, num_iteration=best_iter)\n",
        "    else:\n",
        "        rf = RandomForestRegressor(n_estimators=500, random_state=SEED, n_jobs=-1)\n",
        "        rf.fit(X_tr_rules, y_tr)\n",
        "        oof_rules[va_idx] = rf.predict(X_va)\n",
        "        test_rules_preds[fold_idx-1] = rf.predict(test_rules.values)\n",
        "    #Combine text, audio, and rule-based predictions\n",
        "    base_stack = np.vstack([oof_text[va_idx], oof_audio[va_idx], oof_rules[va_idx]]).T\n",
        "    avg_pred = clip01_5(base_stack.mean(axis=1))\n",
        "    # Compute evaluation metrics for the current fold\n",
        "    mae, rmse, r = metrics(y[va_idx], avg_pred)\n",
        "    print(f\"Fold {fold_idx} base-avg -> MAE {mae:.3f} RMSE {rmse:.3f} r {r:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaaHcrHGh7ci"
      },
      "source": [
        "# Meta Blending\n",
        "Got this idea reading this - [Meta Ensembling](https://medium.com/ml-research-lab/stacking-ensemble-meta-algorithms-for-improve-predictions-f4b4cf3b9237)\n",
        "After training individual text, audio, and rule-based models, this step performs meta-ensembling ,\n",
        "It combines their predictions intelligently.\n",
        "Using NNLS (Non-Negative Least Squares) and rank-based blending, Found the optimal weights for each model’s contribution.\n",
        "Then, isotonic regression is applied for calibration, ensuring final predictions are well-aligned and smooth.\n",
        "\n",
        "[Isotonic Regression](https://stats.stackexchange.com/questions/660622/why-isotonic-regression-for-model-calibration) -- I explored this while I was trying to improve my model and It worked.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVWoIHFyh-Lh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "NNLS value weights: [0.3312 0.3307 0.3381]\n",
            "NNLS VALUE (OOF) -> MAE 0.569 | RMSE 0.767 | r -0.015\n",
            "NNLS rank weights: [0.5395 0.4605 0.    ]\n",
            "RANK-BLEND OOF -> MAE 0.830 | RMSE 1.074 | r -0.020\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# optimal ensemble weights using NNLS (Non-Negative Least Squares)\n",
        "def nnls_sum_to_one(X, y, iters=3000, lr=1e-2):\n",
        "    w = np.ones(X.shape[1], dtype=np.float32) / X.shape[1]\n",
        "    for _ in range(iters):\n",
        "        grad = (2.0 / len(y)) * (X.T @ (X @ w - y))\n",
        "        w = w - lr * grad\n",
        "        w = np.maximum(w, 0.0)\n",
        "        s = w.sum()\n",
        "        if s > 0: w /= s\n",
        "    return w\n",
        "\n",
        "# Stack model predictions (text, audio, rules)\n",
        "X_meta = np.vstack([oof_text, oof_audio, oof_rules]).T\n",
        "y_meta = y_all\n",
        "\n",
        "# Value based NNLS Ensemble :Finds the best non-negative combination of model outputs that minimizes prediction error.\n",
        "w_val = nnls_sum_to_one(X_meta, y_meta)\n",
        "oof_meta_val = clip01_5(X_meta @ w_val)\n",
        "print(\"\\nNNLS value weights:\", np.round(w_val, 4))\n",
        "mae, rmse, r = metrics(y_meta, oof_meta_val)\n",
        "print(f\"NNLS VALUE (OOF) -> MAE {mae:.3f} | RMSE {rmse:.3f} | r {r:.3f}\")\n",
        "\n",
        "#Rank based : Learns weights that best preserve the correct order of predictions.\n",
        "X_meta_rank = np.vstack([rank_scale(oof_text), rank_scale(oof_audio), rank_scale(oof_rules)]).T\n",
        "y_rank = rank_scale(y_meta)\n",
        "w_rank = nnls_sum_to_one(X_meta_rank, y_rank)\n",
        "oof_meta_rank = X_meta_rank @ w_rank\n",
        "print(\"NNLS rank weights:\", np.round(w_rank, 4))\n",
        "\n",
        "#MIximng up both Value based and Rank based\n",
        "alpha = 0.3\n",
        "oof_meta_blend = clip01_5((1 - alpha) * oof_meta_val + alpha * oof_meta_rank)\n",
        "mae, rmse, r = metrics(y_meta, oof_meta_blend)\n",
        "print(f\"RANK-BLEND OOF -> MAE {mae:.3f} | RMSE {rmse:.3f} | r {r:.3f}\")\n",
        "\n",
        "\n",
        "iso = IsotonicRegression(y_min=CLIP_RANGE[0], y_max=CLIP_RANGE[1], out_of_bounds=\"clip\")\n",
        "iso.fit(oof_meta_blend, y_meta)\n",
        "\n",
        "#Apply learned weights to test predictions\n",
        "test_text_m  = test_text_mean.mean(axis=0)\n",
        "test_text_s  = test_text_std.mean(axis=0) + 1e-6\n",
        "test_audio_m = test_audio_mean.mean(axis=0)\n",
        "test_audio_s = test_audio_std.mean(axis=0) + 1e-6\n",
        "test_rules_m = test_rules_preds.mean(axis=0)\n",
        "\n",
        "# Compute test-level weighted ensemble\n",
        "X_test_val = np.vstack([test_text_m, test_audio_m, test_rules_m]).T\n",
        "test_meta_val = clip01_5(X_test_val @ w_val)\n",
        "\n",
        "X_test_rank = np.vstack([rank_scale(test_text_m), rank_scale(test_audio_m), rank_scale(test_rules_m)]).T\n",
        "test_meta_rank = X_test_rank @ w_rank\n",
        "\n",
        "test_meta = clip01_5((1 - alpha) * test_meta_val + alpha * test_meta_rank)\n",
        "\n",
        "# giving more importance to models that are more confident\n",
        "w_conf = np.array([\n",
        "    1.0 / test_text_s.mean(),\n",
        "    1.0 / test_audio_s.mean(),\n",
        "    1.0,  # rules has no std\n",
        "], dtype=np.float32)\n",
        "w_conf = w_conf / w_conf.sum()\n",
        "conf_ens = clip01_5(w_conf[0]*test_text_m + w_conf[1]*test_audio_m + w_conf[2]*test_rules_m)\n",
        "\n",
        "beta = 0.25\n",
        "test_meta = clip01_5((1 - beta) * test_meta + beta * conf_ens)\n",
        "\n",
        "# Final standardization and isotonic calibration\n",
        "tm = (test_meta - test_meta.mean()) / (test_meta.std() + 1e-6)\n",
        "tm = tm * (y_meta.std() + 1e-6) + y_meta.mean()\n",
        "test_meta = clip01_5(0.7 * test_meta + 0.3 * tm)\n",
        "\n",
        "test_meta = clip01_5(iso.predict(test_meta))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_ibnDutlDNG"
      },
      "source": [
        "# SAVING SUBMISSION.CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWQ0kBiTlFF1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved:\n",
            " - output\\stacked_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pd.DataFrame({\n",
        "    \"filename\": test_df[\"filename\"],\n",
        "    \"label\": test_meta\n",
        "}).to_csv(os.path.join(OUT_DIR, \"stacked_predictions.csv\"), index=False)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"oof_text\": oof_text,\n",
        "    \"oof_audio\": oof_audio,\n",
        "    \"oof_rules\": oof_rules,\n",
        "    \"oof_meta_value\": oof_meta_val,\n",
        "    \"oof_meta_rankblend\": oof_meta_blend,\n",
        "    \"label\": y_all,\n",
        "}).to_csv(os.path.join(OUT_DIR, \"oof_features.csv\"), index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" -\", os.path.join(OUT_DIR, \"stacked_predictions.csv\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO0botBbTN3vfpnpNiYebdY",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
