# Audio Regression Project with Multi-Modal Deep Learning

A comprehensive audio regression solution combining traditional feature engineering, deep learning embeddings, and ensemble modeling with multi-modal LLM fine-tuning.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Dataset Structure](#dataset-structure)
- [Installation](#installation)
- [Architecture](#architecture)
- [Usage](#usage)
- [Feature Extraction](#feature-extraction)
- [Model Training](#model-training)
- [Results](#results)
- [Project Structure](#project-structure)

## ğŸ¯ Overview

This project implements an advanced audio regression pipeline that predicts continuous values from audio files. It combines:

- **Ultra-Advanced Audio Features**: 160+ handcrafted features
- **Deep Learning Embeddings**: Wav2Vec2, HuBERT, WavLM
- **Multi-Modal Architecture**: Audio + Text description fusion
- **Ensemble Learning**: XGBoost, LightGBM, CatBoost, RF, ET
- **LoRA Fine-tuning**: Efficient parameter-efficient training

## âœ¨ Features

### Audio Feature Extraction

#### 1. **Ultra-Advanced Features (160 dimensions)**
- **Basic**: Duration (1 feature)
- **Energy**: RMS statistics and temporal dynamics (14 features)
- **Zero-Crossing & Silence**: ZCR analysis and pause detection (10 features)
- **Spectral**: Centroid, bandwidth, rolloff, contrast, flatness, flux (35 features)
- **Pitch & Voice**: F0 analysis, jitter, shimmer, HNR (15 features)
- **MFCCs**: Mean and delta features (24 features)
- **Mel Spectrogram**: Energy distribution and temporal features (15 features)
- **Chroma**: STFT and CQT chroma features (16 features)
- **Rhythm & Tempo**: Onset strength, beat tracking, tempogram (8 features)
- **Harmonic/Percussive**: Separation and ratio analysis (12 features)
- **Audio Quality**: Crest factor, dynamic range, SNR, spectral bands (10 features)

#### 2. **GPU-Accelerated Features (200 dimensions)**
- Optimized feature extraction using PyTorch and torchaudio
- Real-time processing on CUDA-enabled GPUs
- Includes spectral, MFCC, pitch, mel, rhythm, and quality features

#### 3. **Deep Learning Embeddings**
- **Wav2Vec2**: Self-supervised audio representations
- **HuBERT**: Hidden-Unit BERT for speech
- **WavLM**: WavLM for robust audio understanding

#### 4. **Metadata Features (9 dimensions)**
- File size, duration, sample rate
- Amplitude statistics
- Peak characteristics

### Data Augmentation

- **Time Stretching**: Random rate variation (0.95-1.05x)
- **Pitch Shifting**: Random pitch shift (-2 to +2 semitones)
- **Noise Addition**: Gaussian noise injection (Ïƒ=0.005)

### Test Time Augmentation (TTA)

- Multiple augmented predictions averaged
- Configurable number of augmentations (default: 3)
- Improves model robustness

## ğŸ“ Dataset Structure

```
dataset/
â”œâ”€â”€ csvs/
â”‚   â”œâ”€â”€ train.csv          # Training labels (filename, label)
â”‚   â””â”€â”€ test.csv           # Test filenames
â””â”€â”€ audios/
    â”œâ”€â”€ train/             # Training audio files (.wav)
    â””â”€â”€ test/              # Test audio files (.wav)
```

### Data Format

**train.csv**:
```
filename,label
audio_173,3.0
audio_138,3.0
audio_127,2.0
```

**test.csv**:
```
filename
audio_141
audio_114
audio_17
```

## ğŸ”§ Installation

### Requirements

```bash
# Core dependencies
pip install torch torchvision torchaudio
pip install transformers peft
pip install librosa soundfile
pip install numpy pandas scikit-learn scipy
pip install xgboost lightgbm catboost
pip install tqdm joblib
pip install warnings
```

### Kaggle Environment

This notebook is optimized for Kaggle with GPU acceleration:
- Tesla P100 or T4 GPU recommended
- 16GB+ RAM
- Internet access for model downloads

## ğŸ—ï¸ Architecture

### Multi-Modal Regression Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Audio Input   â”‚     â”‚ Text Description â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Wav2Vec2   â”‚         â”‚ DistilBERT  â”‚
  â”‚  Extractor  â”‚         â”‚   Encoder   â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Audio    â”‚           â”‚  Text    â”‚
   â”‚ Proj.    â”‚           â”‚ Features â”‚
   â”‚ (256-d)  â”‚           â”‚ (768-d)  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Fusion   â”‚
            â”‚  Regressor â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Prediction â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ensemble Pipeline

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Audio Files  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                 â”‚
         â–¼                 â–¼                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Traditionalâ”‚     â”‚   Deep   â”‚     â”‚ Metadata â”‚
   â”‚ Features  â”‚     â”‚ Features â”‚     â”‚ Features â”‚
   â”‚ (160-d)   â”‚     â”‚ (1536-d) â”‚     â”‚  (9-d)   â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Feature Pool  â”‚
                  â”‚   (289-d)     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚                â”‚
         â–¼                â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ XGBoost  â”‚     â”‚ LightGBM â”‚     â”‚ CatBoost â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Ensemble  â”‚
                   â”‚  Prediction â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Usage

### Quick Start

```python
# 1. Load and prepare data
import pandas as pd
import numpy as np

train_df = pd.read_csv('csvs/train.csv')
test_df = pd.read_csv('csvs/test.csv')

# 2. Extract features
from feature_extraction import EfficientAudioFeatureExtractor

extractor = EfficientAudioFeatureExtractor()
train_features = extractor.extract_all(train_audio_files)
test_features = extractor.extract_all(test_audio_files)

# 3. Train ensemble models
from models import train_ensemble

models = train_ensemble(train_features, train_labels)

# 4. Make predictions
predictions = ensemble_predict(models, test_features)

# 5. Save submission
submission = pd.DataFrame({
    'filename': test_df['filename'],
    'label': predictions
})
submission.to_csv('submission.csv', index=False)
```

### Advanced Usage: Multi-Modal Training

```python
from multi_modal import train_multimodal_model

# Train with audio + text descriptions
model, best_rmse, predictions = train_multimodal_model()

# Features:
# - Wav2Vec2 audio embeddings
# - DistilBERT text processing
# - LoRA efficient fine-tuning
# - Feature caching
```

## ğŸ” Feature Extraction

### Cell A: Ultra-Advanced Features

```python
# Extract 160 comprehensive audio features
features = extract_ultra_advanced_features(audio_path)

# Features include:
# - Energy (RMS, percentiles, temporal)
# - Spectral (centroid, bandwidth, rolloff, contrast, flatness, flux)
# - Pitch (F0, jitter, shimmer, HNR)
# - MFCCs and deltas
# - Mel spectrogram statistics
# - Chroma features
# - Rhythm and tempo
# - Harmonic/percussive separation
# - Audio quality metrics
```

### Cell B: Feature Combination

```python
# Combine multiple feature sets
combined_features = combine_features(
    mfcc_features,      # 120 features
    ultra_features,     # 160 features
    metadata_features   # 9 features
)  # Total: 289 features
```

### GPU-Accelerated Extraction

```python
# Use GPU for faster processing
features = extract_gpu_features(audio_path)

# Leverages:
# - PyTorch tensors on CUDA
# - torchaudio transforms
# - Parallel processing
```

## ğŸ“ Model Training

### Cross-Validation Strategy

- **5-Fold Cross-Validation**
- **Stratified splits** (if applicable)
- **Random state**: 42 for reproducibility

### Ensemble Models

#### XGBoost
```python
params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'n_estimators': 2000,
    'early_stopping_rounds': 50
}
```

#### LightGBM
```python
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'verbosity': -1
}
```

#### CatBoost
```python
params = {
    'loss_function': 'RMSE',
    'iterations': 2000,
    'learning_rate': 0.05,
    'depth': 6,
    'l2_leaf_reg': 3,
    'verbose': False
}
```

### Multi-Modal Training

```python
# LoRA configuration
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["q_lin", "k_lin", "v_lin"]
)

# Training settings
batch_size = 4
learning_rate = 2e-5
epochs = 15
warmup_ratio = 0.1
```

## ğŸ“Š Results

### Cross-Validation Performance

```
Fold 1: RMSE = 0.6342
Fold 2: RMSE = 0.6177
Fold 3: RMSE = 0.7097
Fold 4: RMSE = 0.6055
Fold 5: RMSE = 0.7607

Mean CV RMSE: 0.6656 Â± 0.0598
OOF RMSE: 0.6680
```

### Model Contributions

- **XGBoost**: Strong baseline performance
- **LightGBM**: Fast training, good generalization
- **CatBoost**: Robust to overfitting
- **Random Forest**: Ensemble diversity
- **Extra Trees**: Additional randomization

### Test Predictions

```
Range: [2.455, 3.887]
Mean: 2.988
Median: 2.951
Std: 0.281
```

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Code.ipynb                       # Main notebook
â”œâ”€â”€ README.md                        # This file
â”‚
â”œâ”€â”€ features/                        # Feature storage
â”‚   â”œâ”€â”€ ultraadvanced_features_train.npz
â”‚   â”œâ”€â”€ ultraadvanced_features_test.npz
â”‚   â”œâ”€â”€ train_features_fixed.npz
â”‚   â”œâ”€â”€ test_features_fixed.npz
â”‚   â”œâ”€â”€ meta_train_feats.npz
â”‚   â”œâ”€â”€ meta_test_feats.npz
â”‚   â”œâ”€â”€ combined_train_v4.npz
â”‚   â””â”€â”€ combined_test_v4.npz
â”‚
â”œâ”€â”€ cache/                           # Model cache
â”‚   â””â”€â”€ audio_*.npy                  # Cached features
â”‚
â””â”€â”€ submissions/
    â”œâ”€â”€ optimized_ensemble_submission.csv
    â””â”€â”€ multimodal_llm_submission.csv
```

## ğŸ”¬ Technical Details

### Audio Processing

- **Sample Rate**: 22,050 Hz (traditional), 16,000 Hz (deep learning)
- **Hop Length**: 512 samples
- **FFT Size**: 2048
- **Mel Bands**: 128
- **MFCCs**: 20-40 coefficients
- **Max Duration**: 45 seconds (padded/trimmed)

### Feature Imputation

- **Method**: Median imputation
- **NaN Handling**: Robust to missing values
- **Scaling**: StandardScaler / RobustScaler

### Memory Optimization

- **Garbage Collection**: Every 25 files
- **GPU Cache**: Cleared periodically
- **Feature Caching**: Disk-based storage
- **Batch Processing**: Configurable batch sizes

## ğŸ¯ Key Innovations

1. **Multi-Scale Feature Extraction**
   - Combines handcrafted and learned features
   - Multiple time scales and representations

2. **GPU Acceleration**
   - PyTorch-based feature extraction
   - 10x faster than CPU-only

3. **Multi-Modal Fusion**
   - Audio embeddings + text descriptions
   - Cross-modal attention mechanisms

4. **LoRA Fine-tuning**
   - Parameter-efficient training
   - Only 2% of parameters trained

5. **Ensemble Diversity**
   - Multiple model architectures
   - Different feature subsets
   - Various hyperparameters

## ğŸ› Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```python
# Reduce batch size
config.batch_size = 2

# Clear cache more frequently
config.gc_frequency = 10
```

**2. Audio Loading Errors**
```python
# Ensure .wav extension
filename = filename if filename.endswith('.wav') else f"{filename}.wav"

# Check audio file integrity
librosa.load(audio_path, sr=None)
```

**3. Feature Dimension Mismatch**
```python
# Verify feature count
assert len(features) == TARGET_FEATURES
```

## ğŸ”„ Workflow

1. **Data Preparation**
   - Load CSV files
   - Verify audio file paths
   - Add .wav extensions if needed

2. **Feature Extraction**
   - Extract ultra-advanced features (Cell A)
   - Generate MFCC features (Cell B)
   - Extract metadata features
   - Combine all features

3. **Model Training**
   - 5-fold cross-validation
   - Train ensemble models
   - Multi-modal training (optional)

4. **Prediction**
   - Test time augmentation
   - Ensemble averaging
   - Generate submission

5. **Submission**
   - Format predictions
   - Remove .wav extensions
   - Save to CSV

## ğŸ“ˆ Performance Tips

### Speed Optimization

- Enable GPU acceleration
- Use cached features
- Reduce TTA augmentations
- Parallelize feature extraction

### Accuracy Improvement

- Increase ensemble diversity
- Add more feature types
- Tune hyperparameters
- Use pseudo-labeling
- Increase TTA augmentations

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Additional feature engineering
- New model architectures
- Hyperparameter optimization
- Data augmentation techniques
- Documentation improvements

## ğŸ“ License

This project is provided for educational and research purposes.

## ğŸ™ Acknowledgments

- **Librosa**: Audio processing library
- **Transformers**: Hugging Face transformers
- **XGBoost/LightGBM/CatBoost**: Ensemble libraries
- **PyTorch**: Deep learning framework
- **Kaggle**: Platform and compute resources

## ğŸ“§ Contact

For questions or issues, please open an issue in the repository.

---

**Last Updated**: December 2025
**Author**: Audio ML Researcher
**Version**: 2.1
